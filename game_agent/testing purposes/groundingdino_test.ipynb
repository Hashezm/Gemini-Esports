{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Grounding DINO Test Notebook\n",
                "Test Grounding DINO on image frames to verify bounding box output for text descriptions."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Installation (run once)\n",
                "```bash\n",
                "git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
                "cd GroundingDINO\n",
                "pip install -e .\n",
                "pip install torch torchvision opencv-python\n",
                "\n",
                "# Download weights\n",
                "mkdir weights\n",
                "wget -O weights/groundingdino_swint_ogc.pth https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
                "import cv2\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration - UPDATE THESE PATHS\n",
                "CONFIG_PATH = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
                "WEIGHTS_PATH = \"weights/groundingdino_swint_ogc.pth\"\n",
                "\n",
                "# Load the model (do this once)\n",
                "model = load_model(CONFIG_PATH, WEIGHTS_PATH)\n",
                "print(\"Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test image path - UPDATE THIS\n",
                "IMAGE_PATH = \"../../img/debug_screenshot.png\"\n",
                "\n",
                "# Text prompt - describe what you want to detect\n",
                "TEXT_PROMPT = \"the player character . health bar . inventory . button .\"\n",
                "\n",
                "# Detection thresholds\n",
                "BOX_THRESHOLD = 0.35\n",
                "TEXT_THRESHOLD = 0.25"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load image using GroundingDINO's helper\n",
                "image_source, image = load_image(IMAGE_PATH)\n",
                "print(f\"Image shape: {image_source.shape}\")\n",
                "\n",
                "# Display original image\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.imshow(image_source)\n",
                "plt.title(\"Original Image\")\n",
                "plt.axis('off')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run inference\n",
                "boxes, logits, phrases = predict(\n",
                "    model=model,\n",
                "    image=image,\n",
                "    caption=TEXT_PROMPT,\n",
                "    box_threshold=BOX_THRESHOLD,\n",
                "    text_threshold=TEXT_THRESHOLD\n",
                ")\n",
                "\n",
                "print(f\"Detected {len(boxes)} objects\")\n",
                "print(f\"\\nBoxes (normalized cx, cy, w, h):\")\n",
                "print(boxes)\n",
                "print(f\"\\nConfidence scores:\")\n",
                "print(logits)\n",
                "print(f\"\\nMatched phrases:\")\n",
                "print(phrases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert normalized boxes to pixel coordinates\n",
                "h, w, _ = image_source.shape\n",
                "\n",
                "pixel_boxes = []\n",
                "for i, (cx, cy, bw, bh) in enumerate(boxes):\n",
                "    x1 = int((cx - bw / 2) * w)\n",
                "    y1 = int((cy - bh / 2) * h)\n",
                "    x2 = int((cx + bw / 2) * w)\n",
                "    y2 = int((cy + bh / 2) * h)\n",
                "    pixel_boxes.append((x1, y1, x2, y2))\n",
                "    print(f\"{phrases[i]}: ({x1}, {y1}) -> ({x2}, {y2})\")\n",
                "\n",
                "print(f\"\\nPixel coordinates (x1, y1, x2, y2):\")\n",
                "for box in pixel_boxes:\n",
                "    print(box)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Annotate and display the image with bounding boxes\n",
                "annotated_frame = annotate(\n",
                "    image_source=image_source,\n",
                "    boxes=boxes,\n",
                "    logits=logits,\n",
                "    phrases=phrases\n",
                ")\n",
                "\n",
                "# Convert BGR to RGB for matplotlib\n",
                "annotated_frame_rgb = annotated_frame[..., ::-1]\n",
                "\n",
                "plt.figure(figsize=(14, 10))\n",
                "plt.imshow(annotated_frame_rgb)\n",
                "plt.title(f\"Detected: {phrases}\")\n",
                "plt.axis('off')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save annotated image\n",
                "output_path = \"annotated_output.jpg\"\n",
                "cv2.imwrite(output_path, annotated_frame)\n",
                "print(f\"Saved annotated image to: {output_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper Function for Easy Reuse"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def detect_with_text(image_path, text_prompt, box_threshold=0.35, text_threshold=0.25, show=True):\n",
                "    \"\"\"\n",
                "    Detect objects in an image using a text prompt.\n",
                "    \n",
                "    Returns:\n",
                "        pixel_boxes: List of (x1, y1, x2, y2) tuples in pixel coordinates\n",
                "        phrases: List of matched text phrases\n",
                "        logits: Confidence scores for each detection\n",
                "    \"\"\"\n",
                "    # Load image\n",
                "    image_source, image = load_image(image_path)\n",
                "    h, w, _ = image_source.shape\n",
                "    \n",
                "    # Run inference\n",
                "    boxes, logits, phrases = predict(\n",
                "        model=model,\n",
                "        image=image,\n",
                "        caption=text_prompt,\n",
                "        box_threshold=box_threshold,\n",
                "        text_threshold=text_threshold\n",
                "    )\n",
                "    \n",
                "    # Convert to pixel coordinates\n",
                "    pixel_boxes = []\n",
                "    for cx, cy, bw, bh in boxes:\n",
                "        x1 = int((cx - bw / 2) * w)\n",
                "        y1 = int((cy - bh / 2) * h)\n",
                "        x2 = int((cx + bw / 2) * w)\n",
                "        y2 = int((cy + bh / 2) * h)\n",
                "        pixel_boxes.append((x1, y1, x2, y2))\n",
                "    \n",
                "    # Optionally display\n",
                "    if show and len(boxes) > 0:\n",
                "        annotated = annotate(image_source, boxes, logits, phrases)\n",
                "        plt.figure(figsize=(12, 8))\n",
                "        plt.imshow(annotated[..., ::-1])\n",
                "        plt.title(f\"Detected: {phrases}\")\n",
                "        plt.axis('off')\n",
                "        plt.show()\n",
                "    \n",
                "    return pixel_boxes, phrases, logits.tolist()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the helper function\n",
                "boxes, phrases, scores = detect_with_text(\n",
                "    \"../../img/debug_screenshot.png\",\n",
                "    \"static UI element . button . text label .\"\n",
                ")\n",
                "\n",
                "for box, phrase, score in zip(boxes, phrases, scores):\n",
                "    print(f\"{phrase} (conf: {score:.2f}): {box}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}